
---
title: "Automate_data_splits_oversampling_v.2"
output:
  html_document:
    toc: true
---


```{r}
%md
Define original dataset
```


```{r}
# Load dependencies
install.packages("SparkR")
library(SparkR)
sparkR.session()

# Define the file path
training_file_path <- "dbfs:/FileStore/tables/ecom_user_data/customer_propensity_training_sample.csv"
testing_file_path <- "dbfs:/FileStore/tables/ecom_user_data/customer_propensity_testing_sample.csv"

# Read the CSV file into a Spark DataFrame
kaggle_training_data_df <- read.df(training_file_path, source = "csv", header = "true", inferSchema = "true")
kaggle_testing_data_df <- read.df(testing_file_path, source = "csv", header = "true", inferSchema = "true")

# Combine datasets
kaggle_total_data <- rbind(kaggle_training_data_df, kaggle_testing_data_df)

# Bring the data from Spark into R memory - only required for SparkR
original_kaggle_total_data <- collect(kaggle_total_data)

# Remove UserID
original_kaggle_total_data <- original_kaggle_total_data[,-1]

# Make sure ordered is a factor
original_kaggle_total_data$ordered <- as.factor(original_kaggle_total_data$ordered)

# Check the structure
str(original_kaggle_total_data) 
```


```{r}
str(kaggle_total_data)
```


```{r}
%md
Define categorical Device variable data
```


```{r}
# Load dependencies
library(SparkR)
sparkR.session()


# Define the file path
training_file_path <- "dbfs:/FileStore/tables/ecom_user_data/customer_propensity_training_sample.csv"
testing_file_path <- "dbfs:/FileStore/tables/ecom_user_data/customer_propensity_testing_sample.csv"

# Read the CSV file into a Spark DataFrame
kaggle_training_data_df <- read.df(training_file_path, source = "csv", header = "true", inferSchema = "true")
kaggle_testing_data_df <- read.df(testing_file_path, source = "csv", header = "true", inferSchema = "true")

# Combine datasets
kaggle_total_data <- rbind(kaggle_training_data_df, kaggle_testing_data_df)

# Bring the data from Spark into R memory - only required for SparkR
kaggle_total_data <- collect(kaggle_total_data)

# Refactor device type variable to be categorical
kaggle_total_data$device_type <- ifelse(
  kaggle_total_data$device_mobile == 1, "mobile",
  ifelse(
    kaggle_total_data$device_computer == 1, "computer",
    "tablet"
  )
)

# Then make sure it's a factor - factor() is similar to as.factor()
kaggle_total_data$device_type <- factor(
  kaggle_total_data$device_type,
  levels = c("mobile", "computer", "tablet")
)

# exlucde UserID
new_kaggle_total_data <- kaggle_total_data[,-c(1, 20, 21, 22)]
# confirm data structure
str(new_kaggle_total_data)
```


```{r}
%md
Define dropped Device variable dataset
```


```{r}
# Load dependencies
install.packages("SparkR")
library(SparkR)
sparkR.session()


dropped_device_kaggle_total_data <- original_kaggle_total_data[,-19]
str(dropped_device_kaggle_total_data)
```


```{r}
%md
Train-test split + oversampling config
```


```{r}
# Load dependencies
library(caret)
library(dplyr)

# Define train-test splits and target variable to manage oversampling
prepare_named_splits <- function(data, 
                                 data_prefix,      
                                 target = "ordered", 
                                 sample_size = NULL,  
                                 seed = 1234) {
  set.seed(seed)
  
  # Factorize target variable
  data[[target]] <- factor(as.vector(data[[target]]), levels = c("0", "1"), labels = c("class0", "class1"))


  # Define stratified sampling
  if (!is.null(sample_size)) {
    sample_index <- createDataPartition(data[[target]], p = sample_size / nrow(data), list = FALSE)
    data <- data[sample_index, ]
    message("Sampled down to ", nrow(data), " rows.")
  }

  # Define split names/sizes
  split_defs <- list("50" = 0.5, "60" = 0.6, "70" = 0.7, "80" = 0.8)

  # Iterate over splits
  for (split_name in names(split_defs)) {
    train_frac <- split_defs[[split_name]]
    test_frac <- 1 - train_frac
    test_name <- as.character(round(test_frac * 100))

    # Partition train/test data
    train_index <- createDataPartition(data[[target]], p = train_frac, list = FALSE)
    train_df <- data[train_index, ]
    test_df  <- data[-train_index, ]

    # Define train control cross validation, repeats and sampling technique
    ctrl <- trainControl(
      method = "repeatedcv",
      number = 4,
      repeats = 2,
      allowParallel = TRUE,
      sampling = "up",
      classProbs = TRUE,
      summaryFunction = twoClassSummary
    )

    # Construct persistent names
    train_var <- paste0(data_prefix, "_train_", split_name)
    test_var  <- paste0(data_prefix, "_test_", test_name)
    ctrl_var  <- paste0(data_prefix, "_ctrl_", split_name)

    assign(train_var, train_df, envir = .GlobalEnv)
    assign(test_var, test_df, envir = .GlobalEnv)
    assign(ctrl_var, ctrl, envir = .GlobalEnv)

    # Manage named output
    message("Created: ", train_var, ", ", test_var, ", ", ctrl_var)
  }
}

```


```{r}
# Apply function to different dataset types

prepare_named_splits(original_kaggle_total_data, "original_kaggle_total_data", sample_size = 60000)

prepare_named_splits(new_kaggle_total_data, "new_kaggle_total_data", sample_size = 60000)

prepare_named_splits(dropped_device_kaggle_total_data, "dropped_device_kaggle_total_data", sample_size = 60000)

```
